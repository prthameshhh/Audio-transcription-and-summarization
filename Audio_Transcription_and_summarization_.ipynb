{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNV0XEZYGDDRkbvQf+E7Qn/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prthameshhh/Audio-transcription-and-summarization/blob/main/Audio_Transcription_and_summarization_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Transcription and Text Summarization with Pre-trained Language Models\n",
        "\n",
        "This Colab notebook demonstrates the usage of pre-trained language models for audio transcription and text summarization tasks. It utilizes the Mistral model for text generation and sets up a Gradio interface for easy user interaction.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The script includes the following components:\n",
        "\n",
        "1. **Model Loading:** It loads the OpenAi whisper(base) model for audio transcription, Mistral 7B pre-trained language model for text generation and sets up the necessary configurations.\n",
        "   \n",
        "2. **Gradio Interface:** The script sets up a Gradio interface to enable users to interactively transcribe audio and generate text summaries.\n",
        "\n",
        "3. **Text Summarization:** Constants, prompt templates, and helper functions are defined for summarizing text. A map-reduce approach is employed for handling larger content efficiently.\n",
        "\n",
        "4. **Text Generation Pipeline:** The script configures the text generation pipeline with specific parameters and quantization settings to ensure optimal performance.\n",
        "\n",
        "5. **Audio Transcription:** A function is defined to transcribe audio files using the OpenAI Whisper model. This functionality is seamlessly integrated into the Gradio interface.\n",
        "\n",
        "## Usage\n",
        "\n",
        "1. **Load Models:** Load the necessary pre-trained language models and configure the text generation pipeline.\n",
        "\n",
        "2. **Define Prompt Templates:** Define constants, prompt templates, and helper functions for text summarization.\n",
        "\n",
        "3. **Setup Gradio Interface:** Set up the Gradio interface for user interaction, allowing users to transcribe audio and receive text summaries.\n",
        "\n",
        "4. **Transcribe Audio and Generate Summaries:** Use the Gradio interface to transcribe audio files and generate concise text summaries interactively.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This Colab notebook provides a comprehensive demonstration of using pre-trained language models for audio transcription and text summarization tasks. By leveraging the Mistral model, OpenAI Whisper model, and Gradio interface, users can efficiently transcribe audio and generate concise summaries with ease.\n",
        "\n"
      ],
      "metadata": {
        "id": "xnqGcmZydZa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install cohere -q\n",
        "!pip install openai -q\n",
        "!pip install git+https://github.com/openai/whisper.git -q\n",
        "!pip install git+https://github.com/openai/whisper.git --use-deprecated=legacy-resolver -q\n",
        "!sudo apt update && sudo apt install ffmpeg -q\n",
        "!pip install langchain torch accelerate bitsandbytes transformers"
      ],
      "metadata": {
        "id": "hKTnlrirTDlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(\"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\")"
      ],
      "metadata": {
        "id": "rF63SuEjT3Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9CeE_NMS395"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import whisper\n",
        "import gradio as gr\n",
        "import langchain\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.cache import InMemoryCache\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALLING THE MODELS"
      ],
      "metadata": {
        "id": "bAHEfw1ybMY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models and tokenizers\n",
        "# Loads the Mistral model and tokenizer for text generation, and sets up quantization configuration.\n",
        "\n",
        "transcriber = whisper.load_model(\"base\")\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, device_map=\"auto\", quantization_config=quantization_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "Y-cLr_pwbYD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREATING THE MODEL"
      ],
      "metadata": {
        "id": "p5ucMQuEcZwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "# Defines various constants used in the script such as style, prompt trigger, output language, model context window, etc.\n",
        "STYLE = \"generate a concise article with title\"\n",
        "PROMPT_TRIGGER = \"ARTICLE GENERATION\"\n",
        "OUTPUT_LANGUAGE = \"English\"\n",
        "VERBOSE = False\n",
        "MODEL_CONTEXT_WINDOW = 13000\n",
        "MAX_ANSWER_TOKENS = 1500\n",
        "CHUNK_SIZE = 10000\n",
        "CHUNK_OVERLAP = 500\n",
        "\n",
        "# Set up pipelines\n",
        "# Configures the text generation pipeline with parameters such as max length, temperature, top p, top k, etc.\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_4bit,\n",
        "    tokenizer=tokenizer,\n",
        "    use_cache=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1000,\n",
        "    max_new_tokens=1000,\n",
        "    do_sample=True,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    temperature=0.2,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    repetition_penalty=1.1,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)\n",
        "\n",
        "# Prompt templates\n",
        "# Defines templates for prompts used in summarization.\n",
        "combine_prompt_template = \"\"\"\n",
        "write a article by taking the following text as a context and\n",
        "{style}\n",
        "\n",
        "```{content}```\n",
        "\n",
        "{trigger} in {language}:\n",
        "\"\"\"\n",
        "\n",
        "map_prompt_template = \"\"\"\n",
        "Write a detailed article using following :\n",
        "\n",
        "{text}\n",
        "\n",
        "detailed article in {language}:\n",
        "\"\"\"\n",
        "\n",
        "# Helper functions\n",
        "def summarize_base(llm, content):\n",
        "    \"\"\"Summarize whole content at once. The content needs to fit into model's context window.\"\"\"\n",
        "    prompt = PromptTemplate.from_template(combine_prompt_template).partial(\n",
        "        style=STYLE, trigger=PROMPT_TRIGGER, language=OUTPUT_LANGUAGE\n",
        "    )\n",
        "    chain = LLMChain(llm=llm, prompt=prompt, verbose=VERBOSE)\n",
        "    output = chain.run(content)\n",
        "    return output\n",
        "\n",
        "def summarize_map_reduce(llm, content):\n",
        "    \"\"\"Summarize content potentially larger that model's context window using map-reduce approach.\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
        "    )\n",
        "    split_docs = text_splitter.create_documents([content])\n",
        "    print(\n",
        "        f\"Map-Reduce content splits ({len(split_docs)} splits): {[len(sd.page_content) for sd in split_docs]}\"\n",
        "    )\n",
        "\n",
        "    map_prompt = PromptTemplate.from_template(map_prompt_template).partial(\n",
        "        language=OUTPUT_LANGUAGE\n",
        "    )\n",
        "\n",
        "    combine_prompt = PromptTemplate.from_template(combine_prompt_template).partial(\n",
        "        style=STYLE, trigger=PROMPT_TRIGGER, language=OUTPUT_LANGUAGE\n",
        "    )\n",
        "\n",
        "    chain = load_summarize_chain(\n",
        "        llm=llm,\n",
        "        chain_type=\"map_reduce\",\n",
        "        map_prompt=map_prompt,\n",
        "        combine_prompt=combine_prompt,\n",
        "        combine_document_variable_name=\"content\",\n",
        "        verbose=VERBOSE,\n",
        "    )\n",
        "\n",
        "    output = chain.run(split_docs)\n",
        "    return output\n",
        "\n",
        "def process_and_generate_audio(audio_file):\n",
        "    \"\"\"Transcribes audio, generates a summary, and creates TTS audio for the summary.\"\"\"\n",
        "    # Transcribe audio\n",
        "    text = transcriber.transcribe(audio_file)[\"text\"]\n",
        "    length = len(text.split(\" \"))\n",
        "\n",
        "    # Summarize based on content tokens\n",
        "    content_tokens = llm.get_num_tokens(text)\n",
        "    base_threshold = 0.75 * MODEL_CONTEXT_WINDOW\n",
        "\n",
        "    if content_tokens < base_threshold:\n",
        "        summary = summarize_base(llm, text)\n",
        "    else:\n",
        "        summary = summarize_map_reduce(llm, text)\n",
        "    len_of_sum = len(summary.split(\" \"))\n",
        "\n",
        "    return (\n",
        "        f\"Transcription:\\n{text}\\n\\n length_of_transcription:\\n {length}\",\n",
        "        f\" {summary}\\n\\n length_of_summary:\\n {len_of_sum}\",\n",
        "    )\n",
        "\n",
        "# Gradio interface\n",
        "# Sets up the Gradio interface for audio transcription and summary generation.\n",
        "iface = gr.Interface(\n",
        "    fn=process_and_generate_audio,\n",
        "    inputs=gr.Audio(type=\"filepath\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Transcription\"),\n",
        "        gr.Textbox(label=\"Summary\"),\n",
        "    ],\n",
        "    live=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "jpgLneMdS4_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface.launch()"
      ],
      "metadata": {
        "id": "a2RnDy-kS482"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}